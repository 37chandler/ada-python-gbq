{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0147b6",
   "metadata": {},
   "source": [
    "## Dram Data Upload\n",
    "\n",
    "In this assignment, you'll upload Dram shop data to your GBQ account and run a couple of queries against it. The data for this assignment is in a .zip file called `dram-items.zip` on Moodle (though the underlying file will have a date attached to it). These are item-level reports exported from the point-of-sale system at the Dram shop.\n",
    "\n",
    "Your goals are to upload these to tables in a GBQ data set. Your tables should have the naming convention `dram_items_YYYYMM` and each table should be a single month. \n",
    "\n",
    "I recommend extracting the zip file into the repository into a folder called `dram-items`. Make sure _not_ to commit any big data files to your repo.\n",
    "\n",
    "Make your data pipeline idempotent, which means you will be checking for the presence of your tables and, if they exist, deleting them before recreating them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c00799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import janitor\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666e218",
   "metadata": {},
   "source": [
    "### GBQ Set Up\n",
    "\n",
    "In this next section we connect to our GBQ project and list the data sets inside to test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f979ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"/Users/chandler/Dropbox/Teaching/\"\n",
    "service_file = 'umt-msba-037daf11ee16.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'umt-msba' # change this to your project. \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3755665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d458e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85be94b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umt-msba:dram_shop\n",
      "umt-msba:transactions\n",
      "umt-msba:wedge_example\n",
      "umt-msba:wedge_transactions\n"
     ]
    }
   ],
   "source": [
    "for item in client.list_datasets() : \n",
    "    print(item.full_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3069ca0",
   "metadata": {},
   "source": [
    "### Checking for and deleting monthly tables\n",
    "\n",
    "We'll get all the tables in our Dram data set that match our pattern, then delete them. We do not want to accidentally delete the item lookup table that we put in this data set in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f296cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ym_pattern = re.compile(r\"20\\d{4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770eadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"dram_shop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37ec4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted umt-msba.dram_shop.dram_items_202010.\n",
      "Deleted umt-msba.dram_shop.dram_items_202011.\n",
      "Deleted umt-msba.dram_shop.dram_items_202012.\n"
     ]
    }
   ],
   "source": [
    "tables = client.list_tables(dataset_id)  \n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    if ym_pattern.search(table.table_id) : \n",
    "        \n",
    "        full_name = \".\".join([gbq_proj_id,dataset_id,table.table_id])\n",
    "        client.delete_table(full_name, not_found_ok=True)\n",
    "        print(f\"Deleted {full_name}.\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9755e",
   "metadata": {},
   "source": [
    "### Reading in and uploading montly tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef2e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a function to make the year-month from dates. \n",
    "\n",
    "def reformat_date(date_string) :\n",
    "    date_string = datetime.datetime.strptime(date_string,\"%Y-%m-%d\")\n",
    "    return(datetime.date.strftime(date_string,\"%Y%m\"))\n",
    "\n",
    "assert(reformat_date(\"2022-09-20\")==\"202209\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53154c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandler/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/Users/chandler/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4755.45it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6260.16it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5562.74it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2049.00it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9686.61it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3068.25it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6955.73it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8322.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2020-01-01-2021-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandler/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5315.97it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6797.90it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2326.29it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6069.90it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2357.68it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10645.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2018-01-01-2019-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 12052.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2019-01-01-2020-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5592.41it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5584.96it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4928.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 11 tables from items-2015-01-01-2016-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5159.05it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8112.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2017-01-01-2018-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5295.84it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3013.15it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3844.46it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10330.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2016-01-01-2017-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5489.93it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5753.50it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5924.16it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8473.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 12 tables from items-2021-01-01-2022-01-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 8793.09it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 10645.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just uploaded 10 tables from items-2022-01-01-2022-10-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "item_files = os.listdir(\"dram-items\")\n",
    "dollar_fields = ['gross_sales','discounts','net_sales','tax']\n",
    "\n",
    "for file in item_files : \n",
    "    item_year = pd.read_csv(\"dram-items/\" + file)\n",
    "    item_year = janitor.clean_names(item_year)\n",
    "    \n",
    "    item_year['ym'] = item_year['date'].map(reformat_date)\n",
    "\n",
    "    # clean up the dollar signs\n",
    "    # found this code here: # https://stackoverflow.com/questions/32464280/converting-currency-with-to-numbers-in-python-pandas\n",
    "    for field in dollar_fields : \n",
    "        item_year[field] = item_year[field].\\\n",
    "            apply(lambda x: x.replace('$','')).\\\n",
    "            apply(lambda x: x.replace(',','')).\\\n",
    "            astype(np.float64)\n",
    "        \n",
    "    # Some of the years use sku in an unpredictable way, \n",
    "    # so I'm just going to overwrite it. \n",
    "    item_year['sku'] = \"\"\n",
    "\n",
    "    \n",
    "    unique_yms = item_year['ym'].unique()\n",
    "    \n",
    "    uploads = 0\n",
    "    \n",
    "    for ym in unique_yms : \n",
    "        for_upload = item_year.query(f\"ym == '{ym}'\")\n",
    "        for_upload.drop(labels=['ym'],axis=\"columns\",inplace=True)\n",
    "        \n",
    "        table_id = f'dram_shop.dram_items_{ym}'\n",
    "        pandas_gbq.to_gbq(for_upload, table_id, project_id=gbq_proj_id,if_exists=\"replace\")\n",
    "        uploads += 1\n",
    "        \n",
    "    print(f\"Just uploaded {uploads} tables from {file}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60415a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
